---
title: "MHI Cohort Email Analysis"
author: "Jae Yong (Francisco) Lee"
output:
  pdf_document: default
  html_document:
    df_print: paged
  word_document: default
geometry: left=3cm,right=3cm,top=2cm,bottom=2cm
---

```{r, echo=FALSE, message=FALSE, warning=FALSE}
library(knitr)
library(formatR)
# Set to wrap long lines in R
opts_chunk$set(tidy.opts=list(width.cutoff=60),tidy=TRUE)
```

## Master of Health Informatics Cohort Email Tracking and Analysis

# Purpose

The purpose of this analysis is to assess the overloading of MHI listserv and the effect of low correspondence rate or the lack thereof.

## Data Preparation

Emails received between August 8th, 2018 and December 21st, 2018 were imported from the author's email account. From the email, the following attributes were extracted: (1) Date, (2) Subject, (3) Type, (4) From, (5) To, and (6) CC. Note that Outlook client does not have the listserv parent (i.e., listserv sent on behalf of ...) as an attribute that was easily extractable; therefore, it was manually extracted. This distinction is necessary because it provides insight into the sender(s) of the majority of listservs.

From this analysis, the author explored the following attributes:

* Areas of Interest
    + From the email, 
        - is there a particular time of month?
        - is there a particular account?
        - is there a common email topic?
    + Out of the listservs,
        - who sends the majority of them?
        - is there an overlap between them and other email senders?

```{r}
# Set working directory
setwd("~/repos/email_report/")

# Include libraries
library(ggplot2)
library(reshape2)
library(ggpubr)
library(knitr)
library(dplyr)
library(tidytext)
library(janeaustenr)
library(stringr)
library(tidyr)
library(tidytext)
library(ggraph)
library(igraph)
library(wordcloud)
library(scales)

# Set figure theme
theme_set(theme_pubr())

# Import dataset
mhi_email <- read.csv(file="mhi-email_original.csv", header = TRUE, stringsAsFactors = FALSE)

# Sanitize date
mhi_email$date <- format(as.Date(mhi_email$date, "%Y-%m-%d"), format="%m/%d") 

# Check missing data and data description
summary(mhi_email)
sapply(mhi_email,function(x) sum(is.na(x)))
str(mhi_email)
```

## Exploratory Analysis
## Frequency based on:
# (1) Date
The following graph displays the number of emails received by a student on the corresponding date.
```{r}
# Compute the frequency.
df <- mhi_email %>%
  group_by(date) %>%
  summarise(counts = n())
df


# Create bar plot.
ggplot(data=df, aes(x = date, y = counts)) +
  ggtitle("Email Frequency by Overall Date") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = "Date Received", y = "Number of Emails") +
  scale_y_continuous(limits = c(0, 10)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) + 
  theme_pubclean() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

The following graph displays the number of emails received by a student 
```{r}
# Compute the frequency.
df <- mhi_email %>%
  group_by(date) %>%
  summarise(counts = n())
df

# August
df8 <- df[grep("08/", df$date, perl = TRUE, value = FALSE),]
g8 <- ggplot(df8,aes(date, counts)) +
  scale_y_continuous(limits = c(0, 10)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) +
  theme_pubclean() +
  theme(axis.title.x = element_blank(),axis.text.x = element_text(angle=90))

# September
df9 <- df[grep("09/", df$date, perl = TRUE, value = FALSE),]
g9 <- ggplot(df9,aes(date, counts)) +
  scale_y_continuous(limits = c(0, 10)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) +
  theme_pubclean() +
  theme(axis.title.x = element_blank(),axis.text.x = element_text(angle=90))

# October
df10 <- df[grep("10/", df$date, perl = TRUE, value = FALSE),]
g10 <- ggplot(df10,aes(date, counts)) +
  scale_y_continuous(limits = c(0, 10)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) +
  theme_pubclean() +
  theme(axis.title.x = element_blank(),axis.text.x = element_text(angle=90))

# November
df11 <- df[grep("11/", df$date, perl = TRUE, value = FALSE),]
g11 <- ggplot(df11,aes(date, counts)) +
  scale_y_continuous(limits = c(0, 10)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) +
  theme_pubclean() +
  theme(axis.title.x = element_blank(),axis.text.x = element_text(angle=90))

# Stack the months
#gridExtra::grid.arrange(grobs = c(g8, g9), ncol=2)

```

# (2) Sender
The following graph displays the number of emails sent by an account on the corresponding date. Note that the compilation of sender was abbreviated in order to reduce the length of labels.

```{r}
test1 <- as.integer(mhi_email$from %in% unique(mhi_email$from))
test1

x <- as.list(rnorm(10000))
names(x) <- paste("a", 1:length(x), sep = "")
list2env(x , envir = .GlobalEnv)

# Create column
mhi_email$abbrev <- 
relabel <- function(x) {
  # find unique
  # assign count?
  
  mhi_email$abbrev <- as.factor(mhi_email$abbrev)
}

# Table of abbreviations
knitr::kable(unique(mhi_email$from), caption="Abbreviation of Senders")

# Compute the frequency.
sf <- mhi_email %>%
  group_by(from) %>%
  summarise(counts = n())
sf

# Create bar plot.
# 
ggplot(sf, aes(x = from, y = counts)) +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) + 
  theme_pubclean() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))
```

# (4) Listserv Majority
The following graph displays the composition of listserv senders. Given that the total number of listservs received outside of IHPME was three, they were excluded from the selection (two emails were from Aquatics Schedule and one email was from Varsity Magazine).

```{r}
# Extract listservs
listservs <- mhi_email[grep("ihpme-mhi-2018-l@listserv.utoronto.ca", mhi_email$from, perl=TRUE, value=FALSE), ]

# Replace listserv address with empty string from "From"
listservs$from <- sub("(ihpme-mhi-2018-l@listserv.utoronto.ca, )", "", listservs$from)

# Compute the date frequency
lfd <- listservs %>%
  group_by(date) %>%
  summarise (counts = n())

# Create bar plot.
ggplot(data=lfd, aes(x = date, y = counts)) +
  ggtitle("Email Frequency by Date") +
  theme(plot.title = element_text(hjust = 0.5)) +
  labs(x = "Date Received", y = "Number of Emails") +
  geom_bar(fill = "#0073C2FF", stat = "identity") +
  geom_text(aes(label = counts), vjust = -0.3) + 
  theme_pubclean() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1))

# Compute the sender frequency
lfs <- listservs %>%
  group_by(from) %>%
  summarise (counts = n())

# Create bar plot.
```

## Text Analysis
Given that the interest is in identifying the source of the emails, I have merged the count to the email that were "sent on behalf of" for listservs.
mhi_emails() is the tidy data frame of MHI emails, published with two columns: *topic*, which contains the text of the email subject, and *sender*, which contains the name of the email senders as a factor in order of sent date.

The "email sender" is used to find the types of emails sent by the sender (e.g., MHI Events sends out a lot of reminders).
```{r}
## Remove listserv sender with the original sent on behalf of
mhi_email$from <- gsub(".*(listserv.utoronto.ca, )", "", mhi_email$from)

## Extract the sender and topic columns only
sender_topics <- setNames(data.frame(mhi_email$subject, mhi_email$from, stringsAsFactors = FALSE), c("topic", "sender"))

## mutate() is used to annotate a "email_id" to keep track of senders in the original format. 
original_emails <- sender_topics %>%
  group_by(sender) %>%
  mutate(email_id = row_number()) %>%
  ungroup()
original_emails

## Restructure df to one-token-per-row format
tidy_emails <- original_emails %>%
  unnest_tokens(word, topic)
tidy_emails
```

Each line of text in the original data frame was separated into tokens. Now, the data is in one-word-per-row format. Often in text analysis, we want to remove stop words (i.e., words that are not useful for an analysis, typically extremely common words such as "the", "of", "to", etc.). Stop words (kept in the tidytext dataset *stop_words*) will be removed with *anti_join()*.

```{r}
library(tidytext)
data(stop_words)

tidy_emails <- tidy_emails %>%
  anti_join(stop_words)

## Find the most common words in all the emails as a whole.
tidy_emails %>%
  count(word, sort = TRUE)

## The tidy data frame will be piped directly to the ggplot2 package to create a visualization of the most common words.
tidy_emails %>%
  count(word, sort = TRUE) %>%
  filter(n > 10) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col() +
  xlab(NULL) +
  coord_flip()

## Word Frequencies by the sender
frequency <- tidy_emails %>%
  mutate(word = str_extract(word, "[a-z']+")) %>%
  count(sender, word) %>%
  group_by(sender) %>%
  mutate(proportion = n / sum(n)) %>%
  select(-n) %>%
  spread(sender, proportion) %>%
  gather(sender, proportion, `ihpme.mhi.grad@utoronto.ca`:.)

## Plot
ggplot(frequency, aes(x = proportion, y = `Topics`, color = abs(`Topics` - proportion))) +
  geom_abline(color = "gray40", lty = 2) +
  geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
  scale_x_log10(labels = percent_format()) +
  scale_y_log10(labels = percent_format()) +
  scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
  facet_wrap(~sender, ncol = 2) +
  theme(legend.position="none") +
  labs(y = "Topics", x = NULL)

## Wordclouds (The most common words in the email topics)
tidy_emails %>%
  anti_join(stop_words) %>%
  count(word) %>%
  with(wordcloud(word, n, max.words = 110))

## Term Frequency by Sender
## This enables us to get a better sense of what type of emails each sender is sending.
email_topic <- original_emails %>%
  unnest_tokens(word, topic) %>%
  count(sender, word, sort = TRUE) %>%
  ungroup()

total_topic <- email_topic %>%
  group_by(sender) %>%
  summarize(total = sum(n))

email_topic <- left_join(email_topic, total_topic)

email_topic
```

There is one row in this *email_topic* data frame for each word-sender combination; *n* is the number of times that word is used by the sender and *total* is the total words by the sender. I will examine the distribution of *n/total* for each sender, the number of times a word is used by the sender divided by the total number of terms (words) used by that sender. This is exactly what *term frequency* is.

```{r}
ggplot(email_topic, aes(n/total, fill = sender)) +
  geom_histogram(show.legend = TRUE)
```

Based on these frequencies, ___ exhibit similar distributions for IHPME (Awards, Events, MHI Grad, MHI Program, and IHPME) and UTGSU email accounts, with many words that occur rarely and fewer words that occur frequently.

```{r}
## The bind_tf_idf function
email_topic <- email_topic %>%
  bind_tf_idf(word, sender, n)
email_topic

## Examine terms with high tf-idf in emails
email_topic %>%
  select(-total) %>%
  arrange(desc(tf_idf))

```

Here we can see all proper nouns, names that are in fact important in these emails.

## n-grams

```{r}
# Let's look at a visualization for these high tf-idf words.
email_topic %>%
  arrange(desc(tf_idf)) %>%
  mutate(word = factor(word, levels = rev(unique(word)))) %>%
  group_by(sender) %>%
  top_n(20) %>%
  ungroup %>%
  ggplot(aes(word, tf_idf, fill = sender)) + 
  geom_col(show.legend = FALSE) +
  labs(x = NULL, y = "tf-idf") + 
  facet_wrap(~sender, ncol = 5, scales = "free") +
  coord_flip()
  
## Relationships between words: n-grams and correlations
email_bigrams <- original_emails %>%
  unnest_tokens(bigram, topic, token = "ngrams", n = 2)

email_bigrams

# Counting and filtering n-grams
email_bigrams %>%
  count(bigram, sort = TRUE)

## As one might expect, a lot of the most common bigrams are pairs of common words, such as "UTGSU digest" and "Seminar Series".
bigrams_separated <- email_bigrams %>%
  separate(bigram, c("word1", "word2"), sep = " ")

bigrams_filtered <- bigrams_separated %>%
  filter(!word1 %in% stop_words$word) %>%
  filter(!word2 %in% stop_words$word)

# new bigram counts:
bigram_counts <- bigrams_filtered %>%
  count(word1, word2, sort = TRUE)

bigram_counts
```

We can see that "UTGSU digest" and "Seminar series" are the most common pairs in the email topics.

```{r}
## Will use "separate/filter/count/unite" to find the most common bigrams not containing stop-words.
bigrams_united <- bigrams_filtered %>%
  unite(bigram, word1, word2, sep = " ")

bigrams_united

## In other analyses, we may be interested in the most common trigrams.
original_emails %>%
  unnest_tokens(trigram, topic, token = "ngrams", n = 3) %>%
  separate(trigram, c("word1", "word2", "word3"), sep = " ") %>%
  filter(!word1 %in% stop_words$word,
         !word2 %in% stop_words$word,
         !word3 %in% stop_words$word) %>%
  count(word1, word2, word3, sort = TRUE)

## Bigrams Analysis
## Find the most common "event/seminar/series/reminder" mentioned by each sender
bigrams_filtered %>%
  filter(word2 == "event") %>%
  count(sender, word1, sort = TRUE)

bigrams_filtered %>%
  filter(word2 == "seminar") %>%
  count(sender, word1, sort = TRUE)

bigrams_filtered %>%
  filter(word2 == "series") %>%
  count(sender, word1, sort = TRUE)

bigrams_filtered %>%
  filter(word2 == "reminder") %>%
  count(sender, word1, sort = TRUE)
```

### Visualization of a Network of Bigram

```{r}
# original counts
bigram_counts

# filter for only relatively common combinations
bigram_graph <- bigram_counts %>%
  filter(n > 3) %>%
  graph_from_data_frame()

bigram_graph

set.seed(2017)

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link() +
  geom_node_point() +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1)

```

Common bigrams in email topics, showing those that occurred more than 3 times and where neither word was a stop word.

```{r}
set.seed(2016)

a <- grid::arrow(type = "closed", length = unit(.15, "inches"))

ggraph(bigram_graph, layout = "fr") +
  geom_edge_link(aes(edge_alpha = n), show.legend = FALSE, arrow = a, end_cap = circle(.07, 'inches')) +
  geom_node_point(color = "lightblue", size = 5) +
  geom_node_text(aes(label = name), vjust = 1, hjust = 1) +
  theme_void()

## Common bigrams in email topics with some polishing
```

# Discussion

Based on the data exploration, there are redundancies in the content of listservs. The frequency at which event reminders are sent overloads the receiving end.

In addition, the ____

This analysis does not include Quercus notifications. As such, the actual volume of email for an average MHI student is expected to be higher.

# Recommendations

1. Reduce number of reminders
    + The reminder emails for an event or the topic of interest could be reduced to every other week instead of every 3 days.
    + The digest could have the content instead of sending out separate emails for each reminder.
2. Aggregate the event series.
    + The author expects that the "read rate" will decrease. As such, this decision should be considered with the decrease in reaching the audience.